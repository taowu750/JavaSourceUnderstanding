# 1. Unicode和UCS<sup id="a1">[\[1\]](#f1)</sup>

`ASCII`是美国标准信息交换码的缩写，`ASCII`规定用7位二进制数字来表示英文字符，
`ASCII`被定为国际标准之后的代号为`ISO-646`.由于`ASCII`码只使用了7个二进制位，
也就是说一个字节可以表示的256个数字中，它仅使用了0~127这128个码位，
剩下的128个码位便可以用来做扩展，用来表 示一些特定语言所独有的字符，
因此对这多余的128个码位的不同扩展，就形成了一系列`ISO-8859-*`的标准。
例如为英语作了专门扩展的字符集编码标准编号为`ISO-8859-1`，也叫做`Latin-1`，
为希腊语所作的扩展编号为`ISO-8859-7`等，完整的列表可以参考《Java Internationalization》一书。

整个`Unicode`项目是由多家计算机软件公司，还包括一些出版行业的公司共同发起的。
`Unicode`的目标之一，能够包含世界上所有语言的字符，这个理想，可以说很远大，
但很快被发现仅靠`Unicode`原先的设计无法实现。

`Unicode`的另一个设计目标，对今天影响深远，那就是对所有字符都采用16位编码（即用一个大小不超过2的16次方的整数数字给每个字符编号，
注意从这个意义上也可以看出，**Unicode是一种编码字符集，而非字符集编码**）。说这个设计目标对现今影响深远，完全不是表扬，
因为到后来连`Unicode`的设计者也发现，16位编码仅有65536个码位，远远不能容纳世界上所有的字符，但当意识到这个问题的时候，
`Unicode`大部分的规范已经制定完毕，也有相当程度的普及，完全推倒重来是不现实的。这成了一个遗留问题，
也是**surrogate pair**这种蹩脚解决方案的发端。

在1984年，喜欢以繁多的编号糊弄群众的国际标准化组织`ISO`也开始着手制定解决不同语言字符数量太大问题的解决方案，
这一方案被称为`Universal Character Set`（`UCS`统一字符集），正式的编号是`ISO-10646`。一开始就确定了UCS是一个31位的编码字符集，
虽然后来他们意识到，2的31次方个码位又实在太多了。

1991年，`Unicode`联盟与`ISO`的工作组终于开始讨论`Unicode`与`UCS`的合并问题，虽然其后的合并进行了很多年，
`Unicode`初版规范中的很多编码都需要被改写，`UCS`也需要对码空间的使用进行必要限制，但成果是喜人的。最终，两者统一了抽象字符集
（即任何一个在`Unicode`中存在的字符，在`UCS`中也存在），且最靠前的**65535**个字符也统一了字符的编码。对于码空间，
两者同意以**一百一十万**为限（即两者都认为虽然65536不够，但2的31次方又太大，一百一十万是个双方都可接受的码空间大小，也够用，
当然，这里说的一百一十万只是个约数），`Unicode`将码空间扩展到了一百一十万，而`UCS`将永久性的不使用一百一十万以后的码位。
也就是说，现在再讲`Unicode`只包含65536个字符是不对的。除了对已经定义的字符进行统一外，
`Unicode`联盟与`ISO`工作组也同意今后任何的扩展工作两者均保持 同步，因此虽然从历史的意义上讲`Unicode`与`UCS`不是一回事
（甚至细节上说也不是一回事），但现在提起`Unicode`，指代两者均无不妥。

# 2. 编码字符集与字符集编码

需要再一次强调的是，无论历史上的`UCS`还是现如今的`Unicode`，两者指的都是**编码字符集**，而不是**字符集编码**。

## 2.1 抽象字符集

一个**抽象字符集**其实就是指字符的集合，例如所有的英文字母是一个抽象字符集，所有的汉字是一个抽象字符集，当然，
把全世界所有语言的符号都放在一起，也可以称为一个抽象字符集，所以这个划分是相当人为的。之所以说“抽象” 二字，
是因为这里所提及的字符不是任何具体形式的字符，都是指唯一存在的抽象字符，而忽略它的具体表现形式。

## 2.2 编码字符集

抽象字符集中的诸多字符，没有顺序之分，谁也不能说哪个字符在哪个字符前面，而且这种抽象字符只有人能理解。
在给一个抽象字符集合中的每个字符都分配一个整数编号之后（注意这个整数并没有要求大小），这个字符集就有了顺序，
就成为了**编码字符集**。同时，通过这个编号，可以唯一确定到底指的是哪一个字符。当然，对于同一个字符，
不同的字符集编码系统所制定的整数编号也不尽相同， 例如“儿”这个字，在`Unicode`中，它的编号是0x513F，
（为方便起见，以十六进制表示，但这个整数编号并不要求必须是以十六进制表示）意思是说，它是Unicode这个编码字符集中的第0x513F个字符。
而在另一种编码字符集比如`Big5`中，这个字就是第0xA449个字符了。这种情况的另一面是，
许多字符在不同的编码字符集中被分配了相同的整数编号，例如英文字母“A”，在`ASCII`及`Unicode`中，
均是第0x41个字符。我们常说的`Unicode`字符集，指的就是这种被分配了整数编号的字符集合，但要澄清的是，
编码字符集中字符被分配的整数编号，不一定就是该字符在计算机中存储时所使用的值，计算机中存储的字符到底使用什么二进制整数值来表示，
是由下面将要说到的字符集编码决定的。

## 2.3 字符集编码

**字符集编码**决定了如何将一个字符的整数编号对应到一个二进制的整数值，有的编码方案简单的将该整数值直接作为其在计算机中的表示而存储，
例如英文字符就是这样，几乎所有的字符集编码方案中，英文字母的整数编号与其在计算机内部存储的二进制形式都一致。但有的编码方案，
例如适用于`Unicode`字符集的`UTF-8`编码形式，就将很大一部分字符的整数编号作了变换后存储在计算机中。以“汉”字为例，
“汉”的`Unicode`值为0x6C49，但其编码为`UTF-8`格式后的值为0xE6B189（注意到变成了三个字节）。我们经常听说的另一种编码方案`UTF-16`，
则对`Unicode`中的前65536个字符编号都不做变换，直接作为计算机存储时使用的值（对65536以后的字符，仍然要做变换），
例如“汉”字的`Unicode`编号为0x6C49，那么经过`UTF-16`编码后存储在计算机上时，它的表示仍为0x6C49！。正是因为`UTF-16`的存在，
使得很多人认为`Unicode`是一种编码（实际上，是一个字符集，再次重申），也因此，很多人说`Unicode`的时候，
他们实际上指的是`UTF-16`.`UTF-16`提供了**surrogate pair**机制，使得`Unicode`中码位大于65536的那些字符得以表示。

`surrogate pair`机制在目前来说实在不常用，甚至连一些`UTF-16`的实现都不支持，其基本的思想就是用两个16位的编码表示一个字符
（注意，只对码位超过65536的字符这么做）。`Unicode`如此死抱着16这个数字不放，有历史的原因，也有实用的原因。

当然还有一种最强的编码，`UTF-32`，他对所有的`Unicode`字符均不做变换，直接使用编号存储！（俗称的以不变应万变），
只是这种编码方案太浪费存储空间（就连1个字节就可以搞定的英文字符，它都必须使用4个字节），因而尽管使用起来方便（不需要任何转换），
却没有得到普及。

`UCS`也有自己的字符集编码：`UCS-2`与`UCS-4`。`UCS-4`与`UTF-32`除了名字不同以外，思想完全一样。
而`UCS-2`与`UTF-16`在对前65536个字符的处理上也完全相同，唯一的区别只在于`UCS-2`不支持`surrogate pair`机制，即是说，
`UCS-2`只能对前65536个字符编码，对其后的字符毫无办法。不过现在再谈起字符编码的时候，
`UCS-2`与`UCS-4`早已成为计算机史学家才会用到的词汇，就让它们继续留在故纸堆里吧。

# 3. Unicode 基本概念

## 3.1 码空间

现在的`Unicode`码空间为U+0000到U+10FFFF，一共1114112个码位，其中只有1112064个码位是合法的（有2048个码位不合法），
但并不是说现在的`Unicode`就有这么多个字符了，实际上其中很多码位还是空闲的，到`Unicode 4.0`规范为止，
只有96382个码位被分配了字符（但无论如何，仍比很多人认为的65536个字符要多得多了）。
其中U+0000 到U+FFFF的部分被称为**基本多语言面（Basic Multilingual Plane，BMP）**，也叫做叫做 0 号平面。
U+10000及以上的字符称为补充字符，这里面其他 16 个平面叫做辅助平面。在`Java`中（`Java1.5`之后），
补充字符使用两个`char`型变量来表示，这两个`char`型变量就组成了所谓的`surrogate pair`（在底层实际上是使用一个`int`进行表示的）。
第一个`char`型变量的范围称为**高代理部分（high-surrogates range）**，从uD800到uDBFF，共1024个码位；
第二个char型变量的范围称为**低代理部分(low-surrogates range）**，从uDC00到uDFFF，共1024个码位。
D800-DBFF可表示的编码范围有10位，DC00-DFFF可表示的编码范围也有10位，加起来就是20位（00000-FFFFF）。
这样使用`surrogate pair`可以表示的字符数一共是1024的平方计1048576个，加上BMP的65536个码位，
去掉2048个非法的码位，正好是1112064个码位。

关于`Unicode`的码空间实际上有一些稍不小心就会让人犯错的地方。比如我们都知道从U+0000到U+FFFF的部分被称为基本多语言面，
这个范围内的字符在使用`UTF-16`编码时，只需要一个`char`型变量就可以保存。仔细看看这个范围，应该有65536这么大，
因此你会说单字节的`UTF-16`编码能够表示65536个字符，你也会说`Unicode`的基本多语言面包含65536个字符，
但是再想想刚才说过的`surrogate pair`，一个`UTF-16`表示的增补字符（再一次的，需要两个`char`型变量才能表示的字符）
怎样才能被正确的识别为增补字符，而不是两个普通的字符呢？答案就是通过看它的第一个`char`是不是在高代理范围内，
第二个`char`是不是在低代理范围内来决定，这也意味着，高代理和低代理所占的共2048个码位（从0xD800到0xDFFF）是不能分配给其他字符的。

这是对`UTF-16`这种编码方法而言，而对`Unicode`这样的字符集呢？在`Unicode`的编号中，U+D800到U+DFFF是否有字符分配？
答案是也没有！这是典型的字符集为方便编码方法而做的安排（你问他们这么做的目的？
当然是希望基本多语言面中的字符和一个`char`型的`UTF-16`编码的字符能够一一对应，少些麻烦，
从中我们也能看出`UTF-16`与`Unicode`间很深的渊源与结合）。也就是说，无论`Unicode`还是`UTF-16`编码后的字符，
在0x0000至0xFFFF这个范围内，只有63488个字符。这就好比最初的CPU被勉强拿来做多媒体应用，用得多了，
CPU就不得不修正自己从硬件上对多媒体应用提供支持了。

`UTF-16`代码点的编码方法：
1. 如果代码点位于 0x000000 - 0x00ffff，直接进行二进制编码，位数不够的左边充 0。
2. 如果代码点位于 0x010000 - 0x10ffff，则：
    - 代码点减去 0x10000，会得到一个位于 0x000000 和 0x0fffff 之间的数字。
    - 这个数字转换为 20 位二进制数，位数不够的，左边充 0，记作：yyyy yyyy yyxx xxxx xxxx。
    - 取出 yy yyyyyyyy，并加上 11011000 00000000(0xD800)，得到高位代理。
    - 取出 xx xxxxxxxx，并加上 11011100 00000000(0xDC00)，得到低位代理。
    - 高位代理和低位代理相连，得到 110110yy yyyyyyyy 110111xx xxxxxxxx。

解析方法反过来就是。解析时如果代理不成对，计算机通常不显示该代理字符。

## 3.2 代码点和代码单元

**代码点 Code Point** 就是指`Unicode`中为字符分配的编号，一个字符只占一个代码点，例如我们说到字符“汉”，它的代码点是U+6C49.
**代码单元 Code Unit** 则是针对编码方法而言，它指的是编码方法中对一个字符编码以后所占的最小存储单元。例如`UTF-8`中，
代码单元是一个字节，因为一个字符可以被编码为1个，2个或者3个4个字节；在`UTF-16`中，代码单元变成了两个字节（就是一个`char`），
因为一个字符可以被编码为1个或2个`char`。说得再罗嗦一点，一个字符，仅仅对应一个代码点，但却可能有多个代码单元（即可能被编码为2个`char`）。

## 3.3 UTF-8

`UTF-8`使用多个字节的序列对编码`Unicode`代码点进行编码。`UTF-8` 的编码规则很简单，只有二条：
1. 对于单字节的符号，字节的第一位设为0，后面7位为这个符号的`Unicode`码。因此对于英语字母，`UTF-8`编码和`ASCII`码是相同的。
2. 对于n字节的符号（n > 1），第一个字节的前n位都设为1，第n + 1位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，
全部为这个符号的`Unicode`码。

下表总结了编码规则，字母x表示可用编码的位：

| `Unicode`符号范围（16进制） | `UTF-8`编码方式（2进制） |
| :------------------------: | :--------------------- |
| 0000 0000 - 0000 007F | 0xxxxxxx |
| 0000 0080 - 0000 07FF | 110xxxxx 10xxxxxx |
| 0000 0800 - 0000 FFFF | 1110xxxx 10xxxxxx 10xxxxxx |
| 0001 0000 - 0010 FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx |  

跟据上表，解读`UTF-8`编码非常简单。如果一个字节的第一位是0，则这个字节单独就是一个字符；如果第一位是1，则连续有多少个1，
就表示当前字符占用多少个字节。

例如“严”的`Unicode`是4E25（100111000100101），根据上表，可以发现4E25处在第三行的范围内（0000 0800 - 0000 FFFF），
因此“严”的`UTF-8`编码需要三个字节，即格式是1110xxxx 10xxxxxx 10xxxxxx。然后，从“严”的最后一个二进制位开始，
依次从后向前填入格式中的x，多出的位补0。这样就得到了，“严”的`UTF-8`编码是11100100 10111000 10100101，转换成十六进制就是E4B8A5。

### 3.3.1 Little endian 和 Big endian

`UCS-2`格式可以存储`Unicode`码（码点不超过0xFFFF）。以汉字“严”为例，`Unicode`码是4E25，需要用两个字节存储，一个字节是4E，
另一个字节是25。存储的时候，4E在前，25在后，这就是`Big endian`方式；25在前，4E在后，这是`Little endian`方式。

第一个字节在前，就是"大头方式"（Big endian），第二个字节在前就是"小头方式"（Little endian）。

`Unicode`规范定义，每一个文件的最前面分别加入一个表示编码顺序的字符，这个字符的名字叫做**零宽度非换行空格（zero width no-break space）**，
用FEFF表示。这正好是两个字节，而且FF比FE大1。如果一个文本文件的头两个字节是FE FF，就表示该文件采用大头方式；
如果头两个字节是FF FE，就表示该文件采用小头方式。

# 4. GB2312，GBK与中文网页

`GB2312`最初指的是一个编码字符集， 其中包含了`ASCII`所包含的英文字符，同时加入了6763个简体汉字以及其他一些`ASCII`之外的符号。
与`Unicode`有`UTF-8`和`UTF-16`一 样（当然， `UTF-8`和`UTF-16`也没有被限定只能用来对`Unicode`进行编码，实际上，
你用它对视频进行编码都是可以的，只是编出的文件没有播放器支持罢了，哈哈），`GB2312`也有自己的编码方案，
但这个方案直接使用一个字符在`GB2312`中的编号作为存储值（与`UTF-32`的做法类似），也因此，这个编码方案甚至没有正式的名称。
我们日常说起`GB2312`的时候，常常即指这个字符集，也指这种编码方案。

`GBK`是`GB2312`的后续标准，添加了更多的汉字和特殊符号，类似的是，`GBK`也是同时指他的字符集和他的编码。
[GBK还是现如今中文Windows操作系统的系统默认编码][charset-encoding]（这正是几乎所有网页上的，文件里的乱码问题的根源）。

说到`GB2312`和`GBK`就不得不提中文网页的编码。尽管很多新开发的Web系统和新上线的注重国际化的网站都开始使用`UTF-8`，
仍有相当一部分的中文媒体坚持使用`GB2312`和`GBK`，例如新浪的页面。其中有两点很值得注意。

第一，页面中`meta`标签的部分，常常可以见到`charset=GB2312`这样的写法，很不幸的是，
这个“charset”其实是用来指定页面使用的是什么字符集编码，而不是使用什么字符集。例如你见到过有人写“charset=UTF-8”，
见到过有人写“charset=ISO-8859-1”，但你见过有人写“charset=Unicode”么？当然没有，因为`Unicode`是一个字符集，而不是编码。
然而正是`charset`这个名称误导了很多程序员，真的以为这里要指定的是字符集，也因而使他们进一步的误以为`UTF-8`和`UTF-16`是一种字符集！
好在`XML`中已经做出了修改，这个位置改成了正确的名称：`encoding`.

第二，页面中说的`GB2312`，实际上并不真的是`GB2312`。我们来做个实验，例如找一个GB2312中不存在的汉字“亸”
（这个字确实不在`GB2312`中，你可以到`GB2312`的码表中去找，保证找不到），这个字在`GBK`中。然后你把它放到一个`html`页面中，
试着在浏览器中打开它，然后选择浏览器的编码为“GB2312”，看到了什么？它完全正常显示！结论不用我说你也明白了，
浏览器实际上使用的是`GBK`来显示。

新浪的页面中也有很多这样的例子，到处都写`charset=GB2312`，却使用了无数个`GB2312`中并不存在的字符。
这种做法对浏览器显示页面并不成问题，但在需要程序抓取页面并保存的时候带来了麻烦，程序将不能依据页面所“声称”的编码进行读取和保存，
而只能尽量猜测正确的编码。

# 5. 网页文件的编码

接着上节的思路说，一个网页要想在浏览器中能够正确显示，需要在三个地方保持编码的一致：**网页文件，网页编码声明和浏览器编码设置**。

首先是网页文件本身的编码，即网页文件在被创建的时候使用什么编码来保存。这个完全取决于创建该网页的人员使用了什么编码保存，
而进一步的取决于该人员使用的操作系统。例如我们使用的中文版`WindowsXP`系统，当你新建一个文本文件，写入一些内容，
并按下<kbd>ctrl</kbd>+<kbd>s</kbd>进行保存的那一刻，操作系统就替你使用`GBK`编码将文件进行了保存（没有使用`UTF-8`，
也没有使用`UTF-16`）。而使用了英文系统的人，系统会使用`ISO-8859-1`进行保存，这也意味着，在英文系统的文件中如果输入一个汉字，
是无法进行保存的（当然，你甚至都无法输入）。 

一个在创建`XML`文件时（创建`HTML`的时候倒很少有人这么认为）常见的误解是以为只要在页面的`encoding`部分声明了`UTF-8`，
则文件就会被保存为`UTF-8`格式。这实在是……怎么说呢，不能埋怨大家。实际上`XML`文件中`encoding`部分与`HTML`文件中的`charset`中一样，
只是告诉“别人”（这个别人可能是浏览你的页面的人，可能是浏览器，也可能是处理你页面的程序，别人需要知道这个，因为除非你告诉他们，
否则谁也猜不出你用了什么编码，仅通过文件的内容判断不出使用了什么编码，这是真的）这个文件使用了什么编码，唯独操作系统不会搭理，
它仍然会按自己默认的编码方式保存文件（再一次的，在我们的中文`WindowsXP`系统中，使用`GBK`保存）。
至于这个文件是不是真的是`encoding`或者`charset`所声明的那种编码保存的呢？答案是不一定！

这就是我们所说的第二个位置，网页编码声明中的编码应该与网页文件保存时使用的编码一致。而浏览器的编码设置实际上并不严格，
就像我们第三节所说的那样，在浏览器中选择使用`GB2312`来查看，它实际上仍然会使用`GBK`进行。而且浏览器还有这样一种好习惯，
即它会尽量猜测使用什么编码查看最合适。

我要重申的是，网页文件的编码和网页文件中声明的编码保持一致，这是一个极好的建议，但如果不一致，
只要网页文件的编码与浏览器的编码设置一致，也是可以正确显示的。 例如有这样一个页面，它使用`GBK`保存，但声明自己是`UTF-8`的。
这个时候用浏览器打开它，首先会看到乱码，因为这个页面“告诉”浏览器用`UTF-8`显示，浏览器会很尊重这个提示，于是乱码一片。
但当手工把浏览器设为`GBK`之后，显示正常。 

# 6. Java 中的字符编码

## 6.1 Java 统一编码字符集

如果你是`JVM`的设计者，让你来决定`JVM`中所有字符的表示形式，你会不会允许使用各种编码方式的字符并存？我想你的答案是不会，
如果在内存中的`Java`字符可以以`GB2312`，`UTF-16`，`BIG5`等各种编码形式存在，那么对开发者来说，连进行最基本的字符串打印、
连接等操作都会寸步难行。例如一个`GB2312`的字符串后面连接一个`UTF-8`的字符串，那么连接后的最终结果应该是什么编码的呢？
你选哪一个都没有道理。

在`Java`中，字符只以一种形式存在，那就是`Unicode`（注意到我们没有选择特定的编码，直接使用它们在字符集中的编号，
这是统一的唯一方法）。但“在Java中”到底是指在哪里呢？就是指在`JVM`中，在内存中，[在你的代码里声明的每一个char，String
类型的变量中][charset-encoding]。

`JVM`的这种约定使得一个字符存在的世界分为了两部分：`JVM`内部和`OS`的文件系统。在`JVM`内部，统一使用`Unicode`表示，
当这个字符被从`JVM`内部移到外部（即保存为文件系统中的一个文件的内容时），就进行了编码转换，使用了具体的编码方案
（也有一种很特殊的情况，使得在`JVM`内部也需要转换，不过这个是后话）。

因此可以说，所有的编码转换就只发生在边界的地方，`JVM`和`OS`的交界处，也就是你的各种输入输出流（或者`Reader`，`Writer`类）
起作用的地方。

## 6.2 Java IO 系统

所有的`IO`基本上可以分为两大阵营：面向字符的`Reader`、`Wrtier`，以及面向字节的输入输出流。

面向字符和面向字节中的所谓“面向”什么，是指这些类在处理输入输出的时候，在哪个意义上保持一致。如果是面向字节，
那么这类工作要保证系统中的文件二进制内容和读入`JVM`内部的二进制内容要一致。不能变换任何0和1的顺序
（也就是文件是怎么存的就怎么取，与文件保存的编码一致）。因此这是一种非常“忠实于原著”的做法.
这种输入输出方式很适合读入视频文件或者音频文件，或者任何不需要做变换的文件内容。

而面向字符的`IO`是指希望系统中的文件的字符和读入内存的“字符”（注意和字节的区别）要一致。例如我们的中文版`WindowsXP`
系统上有一个`GBK`的文本文件，其中有一个“汉”字，这个字的`GBK`编码是0xBABA（而`Unicode`编号是0x6C49），
当我们使用面向字符的`IO`把它读入内存并保存在一个`char`型变量中时，我希望`IO`系统不要傻傻的直接把0xBABA放到这个`char`型变量中，
我甚至都不关心这个`char`型变量具体的二进制内容到底是多少，我只希望这个字符读进来之后仍然是“汉”这个字。

从这个意义上也可以看出，面向字符的`IO`类，也就是`Reader`和`Writer`类，实际上隐式的为我们做了编码转换，
在输出时，将内存中的`Unicode`字符使用系统默认的编码方式进行了编码，而在输入时，
将文件系统中已经编码过的字符使用默认编码方案进行了还原。
我两次提到“默认”，是说`Reader`和`Writer`的聪明也仅此而已了，它们只会使用这个默认的编码来做转换，
你不能为一个`Reader`或者`Writer`指定转换时使用的编码。这也意味着，如果你使用中文版`WindowsXP`系统，
而上面存放了一个`UTF-8`编码的文件，当你使用`Reader`类来读入的时候，它会傻傻的使用`GBK`来做转换，转换后的内容当然驴唇不对马嘴！ 
这种笨，有时候其实是一种傻瓜式的功能提供方式，对大多数初级用户（以及不需要跨平台的高级用户）来说反而是件好事。

所谓编码转换就是一个字符与字节之间的转换，因此`Java`的`IO`系统中能够指定转换编码的地方，也就在字符与字节转换的地方，
那就是`InputStreamReader`和`OutputStreamWriter`<sup id="a2">[\[2\]](#f2)</sup>。 这两个类是字节流和字符流之间的适配器类，
因此他们肩负着编码转换的任务简直太自然啦！要注意，实际上也只能在这两类实例化的时候指定编码。

下面是一段小程序，来把“汉”字用UTF-8编码写到文件中：
```java
try(PrintWriter out = new PrintWriter(new OutputStreamWriter(  
                new FileOutputStream("c:/utf-8.txt"), "UTF-8"));) {  
    out.write("汉"); 
} catch (IOException e) {
    throw new RuntimeException(e);
}
```

## 6.3 Java 中的增补字符

`Java`号称对`Unicode`提供天然的支持，这话在很久很久以前就已经是假的了（不过曾经是真的），实际上，到`JDK5.0`为止，
`Java`才算刚刚跟上`Unicode`的脚步，开始提供对增补字符的支持。

### 6.3.1 分层方案和 API

`Java`对增补字符的处理采用了一种分层的方案，在这种方法中，[一个 char 表示一个 UTF-16 代码单元][charset-encoding]：
1. 使用基本类型`int`在底层`API`中表示代码点，例如`Character`类的静态方法。
2. 将所有形式的`char`序列均解释为`UTF-16`序列，并促进其在更高层级`API`中的使用。
3. 提供`API`，以方便在各种`char`和基于代码点的表示法之间的转换。

新增的低层`API`分为两大类：用于各种`char`和基于代码点的表示法之间转换的方法和用于分析和映射代码点的方法。
最基本的转换方法是`Character.toCodePoint(char high, char low)`（用于将两个`UTF-16`代码单元转换为一个代码点）和
`Character.toChars(int codePoint)`（用于将指定的代码点转换为一个或两个`UTF-16`代码单元，然后封装到一个`char[]`内）。

不过，由于大多数情况下文本以字符序列的形式出现，因此，另外提供`codePointAt`和`codePointBefore`方法，
用于将代码点从各种字符序列表示法中提取出来：`Character.codePointAt(char[] a, int index)`和`String.codePointBefore(int index)`。
在将代码点插入字符序列时，大多数情况下均有一些针对`StringBuffer`和`StringBuilder`类的`appendCodePoint(int codePoint)`方法，
以及一个接受表示代码点的`int[]`的`String`构造器。

几种用于分析代码单元和代码点的方法有助于转换过程：`Character`类中的`isHighSurrogate`和`isLowSurrogate`方法可以识别用于表示增补字符的`char`值；
`charCount(int codePoint)`方法可以确定是否需要将某个代码点转换为一个或两个`char`。

大多数基于代码点的方法对所有`Unicode`字符均能取得和原来基于`char`的旧方法对`BMP`字符所实现的功能。以下是一些典型例子：
1. `Character.isLetter(int codePoint)`可根据`Unicode`标准识别字母。
2. `Character.isJavaIdentifierStart(int codePoint)`可根据`Java`语言规范确定代码点是否可以启动标识符。
3. `Character.UnicodeBlock.of(int codePoint)`可搜索代码点所属的`Unicode`字符子集。
4. `Character.toUpperCase(int codePoint)`可将给定的代码点转换为其大写等值字符。尽管此方法能够支持增补字符，
但是它仍然不能解决根本的问题，即在某些情况下，逐个字符的转换无法正确完成。例如，德文字符“"ß"”应该转换为“SS”，
这需要使用`String.toUpperCase`方法。

注意大多数接受代码点的方法并不检查给定的`int`值是否处于有效的`Unicode`代码点范围之内
（如上所述，只有 0x0 至 0x10FFFF 之间的范围是有效的）。在大多数情况下，该值是以确保其有效的方法产生的，
在这些低层`API`中反复检查其有效性可能会对系统性能造成负面的影响。在无法确保有效性的情况下，
应用程序必须使用`Character.isValidCodePoint`方法确保代码点有效。大多数方法对于无效的代码点采取的行为没有特别加以指定，
不同的实现可能会有所不同。

我们可以使用底层`API`实现自己的功能，比如创建仅含有单个代码点的`String`：
```java
String newString(int codePoint) {
    // 针对 BMP 字符优化
    if (Character.charCount(codePoint) == 1) {
        return String.valueOf((char) codePoint);
    } else {
        return new String(Character.toChars(codePoint));
    }
}
```

下面列出了大部分增补字符的改进：
1. [新的格式化 API 支持增补文字][charset-encoding]。
2. `Java`编程语言中的标识符: `Java`语言规范指出所有`Unicode`字母和数字均可用于标识符。许多增补字符是字母或数字，
因此`Java`语言规范已经参照新的基于代码点的方法进行更新，以在标识符内定义合法字符。为使用这些新方法，
需要检测标识符的`javac`编译器和其他工具都进行了修订。
3. 库内的增补字符支持。许多`J2SE`库已经过增强，可以通过现有接口支持增补字符。以下是一些例子：
    - 字符串大小写转换功能已更新，可以处理增补字符，也可以实现`Unicode`标准中规定的特殊大小写规则。
    - `java.util.regex`包已更新，这样模式字符串和目标字符串均可以包含增补字符并将其作为完整单元处理。
    - 现在，在`java.text`包内进行处理时，会将增补字符看作完整单元。
    - `java.text.Bidi`类已更新，可以处理增补字符和`Unicode 6.2`中新增的其他字符。请注意，
    `Cypriot Syllabary`字符子集内的增补字符具有从右至左的方向性。
    
### 6.3.2 Java 源文件中使用增补字符
    
在`Java`编程语言源文件中，如果使用可以直接表示增补字符的字符编码，则使用增补字符最为方便。`UTF-8`是最佳的选择。
在所使用的字符编码无法直接表示字符的情况下，`Java`编程语言提供一种`Unicode`转义符语法。此语法没有经过增强，
无法直接表示增补字符。而是使用两个连续的`Unicode`转义符将其表示为`UTF-16`字符表示法中的两个编码单元。
例如，字符 U+20000 写作“\uD840\uDC00”。您也许不愿意探究这些转义序列的含义；最好是写入支持所需增补字符的编码，
然后使用一种工具（如 native2ascii）将其转换为转义序列。

### 6.3.3 经修订的 UTF-8

需要特别注意的是，某些`J2SE`接口使用的编码与`UTF-8`相似但与其并不兼容。以前，此编码有时被称为 **Java modified UTF-8**
（经 Java 修订的 UTF-8） 或（错误地）直接称为“UTF-8”。

经修订的`UTF-8`和标准`UTF-8`之间之所以不兼容，其原因有两点：
1. 经修订的`UTF-8`将字符 U+0000 表示为双字节序列 0xC0 0x80，而标准`UTF-8`使用单字节值 0x0。
2. 经修订的`UTF-8`通过对其`UTF-16`表示法的两个代理代码单元单独进行编码表示增补字符 。每个代理代码单元由三个字节来表示，
共有六个字节。而标准`UTF-8`使用单个四字节序列表示整个字符。

| 字节数 | 经修订的`UTF-8`范围（2进制） |
| ------ | -------------------------- |
| 1字节 | 0xxxxxxx |
| 2字节 | 110xxxxx 10xxxxxx | 
| 3字节 | 1110xxxx 10xxxxxx 10xxxxxx |
| 4字节 | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx | 
| 5字节 | 111110xx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx | 
| 6字节 | 1111110x 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx |

`Java`虚拟机及其附带的接口，如`Java`本机接口、`Java``.class`文件，对象序列化格式，
`java.io.DataInput`和`DataOutput`接口及其继承类中（还有使用这些接口和类的实现），使用经修订的`UTF-8`。
`Java`本机接口提供与经修订的`UTF-8`之间进行转换的例程。而标准`UTF-8`由`String`类、
`java.io.InputStreamReader`和`OutputStreamWriter`类、`java.nio.charset`以及许多其上层的`API`提供支持。

由于经修订的`UTF-8`与标准的`UTF-8`不兼容，因此切勿同时使用这两种版本的编码。经修订的`UTF-8`只能与上述的`Java`接口配合使用。
在任何其他情况下，尤其对于可能来自非基于`Java`平台的软件的或可能通过其编译的数据流，必须使用标准的`UTF-8`。
需要使用标准的`UTF-8`时，则不能使用`Java`本机接口例程与经修订的`UTF-8`进行转换。

### 6.3.4 在 Java 中使用增补字符

为了在应用程序中使用增补字符，推荐使用`String`类及其他`CharSequence`接口实现类，而不推荐直接使用`char`。


[article]: http://www.360doc.com/content/12/0420/13/9470897_205152817.shtml
[charset-encoding]: ../../../test/java_/lang/CharsetAndEncodingTest.java

<b id="f1">\[1\]</b> 参考 http://www.360doc.com/content/12/0420/13/9470897_205152817.shtml。 [↩](#a1)  
<b id="f2">\[2\]</b> `PrintStream`也可以对`OutputStream`进行包装并指定编码方式：`PrintStream(OutputStream out, boolean autoFlush,
String encoding)`，但实质上也是调用`OutputStreamWriter`来实现的。[↩](#a2)